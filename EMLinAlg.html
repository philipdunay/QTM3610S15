
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Linear Algebra</title><meta name="generator" content="MATLAB 8.3"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2015-04-06"><meta name="DC.source" content="EMLinAlg.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Linear Algebra</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">Eigenvalues and Eigenvectors</a></li><li><a href="#2">Singular Value Decomposition (SVD)</a></li><li><a href="#3">Transposition</a></li><li><a href="#4">Identity Matrix</a></li><li><a href="#5">Matrix Inverse</a></li><li><a href="#6">Dot Product</a></li><li><a href="#7">Orthogonality</a></li><li><a href="#8">Basis</a></li><li><a href="#9">Symmetric Matrix</a></li><li><a href="#10">Idempotent Matrix</a></li><li><a href="#11">Rank of a Matrix</a></li><li><a href="#12">Linear Combination</a></li><li><a href="#13">Cross Product</a></li><li><a href="#14">Scalar Projection</a></li><li><a href="#15">Positive-definite Matrix</a></li></ul></div><h2>Eigenvalues and Eigenvectors<a name="1"></a></h2><p>Eigenvalues and eigenvectors are characteristic components of square matrices. V is the matrix of eigenvalues and D is the matrix of eigenvectors.</p><pre class="codeinput">[V, D] = eig([1 3; 2 4])
</pre><pre class="codeoutput">V =
        -0.909376709132124        -0.565767464968992
         0.415973557919284        -0.824564840132394
D =
        -0.372281323269014                         0
                         0          5.37228132326901
</pre><h2>Singular Value Decomposition (SVD)<a name="2"></a></h2><p>SVD is considered to be a factorization of a matrix. It results in three components, which can be multiplied together to reform the matrix. Produces two unitary matrices, U and V the same size as the input matrix. S Is a diagonal matrix, with the values in decreasing order. U*S*V' will result in the original matrix.</p><pre class="codeinput">[U, S, V] = svd(rand(3))
</pre><pre class="codeoutput">U =
        -0.567225278101591         0.471929553084925        -0.674935538260978
        -0.494368575054479        -0.850567577230537        -0.179261006810679
        -0.658676852395145         0.231985545862749          0.71577057122348
S =
          1.94503964190219                         0                         0
                         0         0.350940402086211                         0
                         0                         0        0.0822046785160976
V =
        -0.727896180701109        -0.364574311733166        -0.580734639353489
        -0.214239717546836         -0.68361525131189         0.697690140104641
        -0.651358959058821         0.632262413399744         0.419494632928408
</pre><h2>Transposition<a name="3"></a></h2><p>Transposition is to switch the rows and columns of a matrix. It is easily done in MATLAB&reg; using the apostrophe immediately following a matrix: <img src="EMLinAlg_eq58093.png" alt="$A'$"></p><pre class="codeinput">TranEx = [0 100; 4 17]
TranRes = TranEx'
</pre><pre class="codeoutput">TranEx =
     0   100
     4    17
TranRes =
     0     4
   100    17
</pre><h2>Identity Matrix<a name="4"></a></h2><p>A square matrix with a diagonal composed entirely of ones, and all other entries are zeros. The identity matrix will be the result of multiplying a matrix by its inverse. Another feature of the identity matrix is that it will not alter any matrix multiplied by it. In MATLAB&reg;, the function eye(n) creates an identity matrix, where n is the number of rows and columns.</p><pre class="codeinput">eye(3)
EyeEx = [1 2; 3 4] * eye(2)
</pre><pre class="codeoutput">ans =
     1     0     0
     0     1     0
     0     0     1
EyeEx =
     1     2
     3     4
</pre><h2>Matrix Inverse<a name="5"></a></h2><p>A matrix inverse operates in such a way that the inverse of a matrix multiplied by the original matrix results in the identity matrix: <img src="EMLinAlg_eq55849.png" alt="$A^-1 * A = I$">. The original matrix must be square and its determinant must be non-zero: <img src="EMLinAlg_eq29930.png" alt="$1/ad-bc$"> does not equal 0 for a 2x2 matrix.</p><pre class="codeinput">Orig = [1 2; 3 4]
Inverted = Orig^-1
I = round(Orig * Inverted)
</pre><pre class="codeoutput">Orig =
     1     2
     3     4
Inverted =
                        -2                         1
                       1.5                      -0.5
I =
     1     0
     0     1
</pre><h2>Dot Product<a name="6"></a></h2><p>A dot product of two equally-sized vectors yields a scalar. The dot product is taken by multiplying elements of a vector or matrix component-wise. <img src="EMLinAlg_eq57866.png" alt="$u \cdot v = \sum_{i=1}^n u_i v_i + u_1 v_1 + u_2 v_2 ... u_n v_n$"></p><pre class="codeinput">dotProd = dot([1 2 3],[4 5 6])
</pre><pre class="codeoutput">dotProd =
    32
</pre><h2>Orthogonality<a name="7"></a></h2><p>The property of two vectors being perpendicular to eachother. This can be determined by the length of A multiplied by the length of B, times the cosine of the angle between the two vectors. The length of a vector is the square root of the sum of all of its components squared. Orthogonality occurs when the vectors are perpendicular, requiring that the angle between the two is ninety degrees, which, when plugged into cosine results in zero. This is equivalent to stating that the dot product of two vectors is equal to zero. Should the angle need to be calculated, use arccosine to find it, knowing the lengths and vectors. Orthonormality is the case when vectors are orthogonal and have lengths of one. An orthonormal length calculation and orthogonality test is demonstrated below.</p><pre class="codeinput">vecA = [1 0];
vecB = [0 1];
LengthA = sqrt([1^2 + 0^2])
LengthB = sqrt([0^2 + 1^2])
zero = LengthA * LengthB * cosd(90)
</pre><pre class="codeoutput">LengthA =
     1
LengthB =
     1
zero =
     0
</pre><h2>Basis<a name="8"></a></h2><p>A basis is a set of <img src="EMLinAlg_eq25947.png" alt="$n$"> linearly independent vectors in a vector space of n dimensions. Given a vector space, any element in it can be reached via a unique combination the vectors in the basis. As an example, <img src="EMLinAlg_eq03598.png" alt="$X$"> is the vector, while <img src="EMLinAlg_eq14800.png" alt="$a1$"> through <img src="EMLinAlg_eq90244.png" alt="$a3$"> are the bases:</p><pre class="codeinput">X = [5; 12; 13];
a1 = [1; 0; 0];
a2 = [0; 1; 0];
a3 = [0; 0; 1];
XDemo = 5*a1 + 12*a2 + 13*a3
</pre><pre class="codeoutput">XDemo =
     5
    12
    13
</pre><h2>Symmetric Matrix<a name="9"></a></h2><p>Symmetric matrices remain unchanged when they are transposed. A symbolic example is <img src="EMLinAlg_eq47033.png" alt="$A^T = A$">. Below, it should be clear that the two matrices are symmetric.</p><pre class="codeinput">SymMat = [1 0 1; 0 1 0; 1 0 1]
SymMatTrans = SymMat'
</pre><pre class="codeoutput">SymMat =
     1     0     1
     0     1     0
     1     0     1
SymMatTrans =
     1     0     1
     0     1     0
     1     0     1
</pre><h2>Idempotent Matrix<a name="10"></a></h2><p>A matrix which remains unchanged if multiplied by itself. A symbolic example is <img src="EMLinAlg_eq66249.png" alt="$A^2 = A$">. An example of a 2x2 idempotent matrix is below.</p><pre class="codeinput">IdempMat = [1, 1; 0, 0]
IdempMatSq = IdempMat^2
</pre><pre class="codeoutput">IdempMat =
     1     1
     0     0
IdempMatSq =
     1     1
     0     0
</pre><h2>Rank of a Matrix<a name="11"></a></h2><p>Rank indicates the number of independent variables (columns or rows) in a matrix. Can be found using row-reduced echelon form and identifying the pivots (number of rows with leading ones).</p><pre class="codeinput">rrefEx = rref([1 2 3; 4 5 6; 7 8 9])
rankEx = rank([1 2 3; 4 5 6; 7 8 9])
</pre><pre class="codeoutput">rrefEx =
     1     0    -1
     0     1     2
     0     0     0
rankEx =
     2
</pre><h2>Linear Combination<a name="12"></a></h2><p>Any combination of scalars and vectors or their sums. Linear dependence is identified through linear combinations, with linearly independent vectors being unable to be defined by any combination of vectors in a space.</p><h2>Cross Product<a name="13"></a></h2><p>A cross product of two vectors yields a third vector perpendicular (normal) to both original vectors.</p><pre class="codeinput">crossProd = cross([1 2 3],[4 5 6])
</pre><pre class="codeoutput">crossProd =
    -3     6    -3
</pre><h2>Scalar Projection<a name="14"></a></h2><p>This is used to find the length of a vector's "shadow" along the axis it is projecting on. Generally, the method is to mutliply the length of the vector by the cosine of the angle between it and the axis to get the length of the projection along the axis.</p><h2>Positive-definite Matrix<a name="15"></a></h2><p>Positive definite matrices are described by the process <img src="EMLinAlg_eq27502.png" alt="$z^T M z$">, resulting in a matrix with no negative entries nor zeros as entries for any <img src="EMLinAlg_eq88768.png" alt="$z$">. This is another way of saying that the eigendecomposition of the matrix results in only positive eigenvalues. Positive-semidefinite matrices are determined in the same way, but they may contain zeros as entries.</p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Linear Algebra
%% Eigenvalues and Eigenvectors
% Eigenvalues and eigenvectors are characteristic components of square
% matrices. V is the matrix of eigenvalues and D is the matrix of
% eigenvectors.
[V, D] = eig([1 3; 2 4])
%% Singular Value Decomposition (SVD)
% SVD is considered to be a factorization of a matrix. It results in three
% components, which can be multiplied together to reform the matrix.
% Produces two unitary matrices, U and V the same size as the input matrix. S
% Is a diagonal matrix, with the values in decreasing order. U*S*V' will
% result in the original matrix. 
[U, S, V] = svd(rand(3))
%% Transposition
% Transposition is to switch the rows and columns of a matrix. It is easily
% done in MATLAB(R) using the apostrophe immediately following a matrix: 
% $A'$
TranEx = [0 100; 4 17]
TranRes = TranEx'
%% Identity Matrix
% A square matrix with a diagonal composed entirely of ones, and all other
% entries are zeros. The identity matrix will be the result of multiplying a matrix by its inverse.
% Another feature of the identity matrix is that it will not alter any
% matrix multiplied by it.
% In MATLAB(R), the function eye(n) creates an identity
% matrix, where n is the number of rows and columns.
eye(3)
EyeEx = [1 2; 3 4] * eye(2)
%% Matrix Inverse
% A matrix inverse operates in such a way that the inverse of a matrix
% multiplied by the original matrix results in the identity matrix: 
% $A^-1 * A = I$. The original matrix must be square and its determinant
% must be non-zero: $1/ad-bc$ does not equal 0 for a 2x2 matrix.
Orig = [1 2; 3 4]
Inverted = Orig^-1
I = round(Orig * Inverted)
%% Dot Product
% A dot product of two equally-sized vectors yields a scalar.  
% The dot product is taken by multiplying elements of a vector or matrix
% component-wise. $u \cdot v = \sum_{i=1}^n u_i v_i + u_1 v_1 + u_2 v_2 ... u_n v_n$
dotProd = dot([1 2 3],[4 5 6])
%% Orthogonality
% The property of two vectors being perpendicular to eachother. This can be
% determined by the length of A multiplied by the length of B, times the
% cosine of the angle between the two vectors. The length of a vector is
% the square root of the sum of all of its components squared. Orthogonality occurs
% when the vectors are perpendicular, requiring that the angle between the
% two is ninety degrees, which, when plugged into cosine results in zero. 
% This is equivalent to stating that the dot product of two vectors is
% equal to zero. Should the angle need to be calculated, use arccosine to find it, knowing the lengths and vectors. Orthonormality is the case when vectors are orthogonal
% and have lengths of one. An orthonormal length calculation and
% orthogonality test is demonstrated below.
vecA = [1 0];
vecB = [0 1];
LengthA = sqrt([1^2 + 0^2])
LengthB = sqrt([0^2 + 1^2])
zero = LengthA * LengthB * cosd(90)
%% Basis
% A basis is a set of $n$ linearly independent vectors in a vector space of
% n dimensions. Given a vector space, any element in it can be reached via a unique combination the vectors in the basis.
% As an example, $X$ is the vector, while $a1$ through $a3$ are the bases:
X = [5; 12; 13];
a1 = [1; 0; 0];
a2 = [0; 1; 0];
a3 = [0; 0; 1];
XDemo = 5*a1 + 12*a2 + 13*a3
%% Symmetric Matrix
% Symmetric matrices remain unchanged when they are transposed. 
% A symbolic example is $A^T = A$. Below, it should be clear that the two
% matrices are symmetric.
SymMat = [1 0 1; 0 1 0; 1 0 1]
SymMatTrans = SymMat'
%% Idempotent Matrix
% A matrix which remains unchanged if multiplied by itself. 
% A symbolic example is $A^2 = A$. An example of a 2x2 idempotent matrix is
% below.
IdempMat = [1, 1; 0, 0]
IdempMatSq = IdempMat^2
%% Rank of a Matrix
% Rank indicates the number of independent variables (columns or rows) in a matrix.
% Can be found using row-reduced echelon form and identifying the pivots
% (number of rows with leading ones).
rrefEx = rref([1 2 3; 4 5 6; 7 8 9])
rankEx = rank([1 2 3; 4 5 6; 7 8 9])
%% Linear Combination
% Any combination of scalars and vectors or their sums. Linear dependence
% is identified through linear combinations, with linearly independent
% vectors being unable to be defined by any combination of vectors in a
% space.
%% Cross Product
% A cross product of two vectors yields a third vector perpendicular (normal) to both original vectors.
crossProd = cross([1 2 3],[4 5 6])
%% Scalar Projection
% This is used to find the length of a vector's "shadow" along the axis it
% is projecting on. Generally, the method is to mutliply the length of the
% vector by the cosine of the angle between it and the axis to get the
% length of the projection along the axis. 
%% Positive-definite Matrix
% Positive definite matrices are described by the process $z^T M z$,
% resulting in a matrix with no negative entries nor zeros as entries for 
% any $z$. This is another way of saying that the eigendecomposition of 
% the matrix results in only positive eigenvalues.
% Positive-semidefinite matrices are determined in the same way, but
% they may contain zeros as entries.
##### SOURCE END #####
--></body></html>