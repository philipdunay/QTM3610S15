
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Principal Component Analysis and Factor Analysis in SPSS</title><meta name="generator" content="MATLAB 8.3"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2015-04-05"><meta name="DC.source" content="EMPCA.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><h1>Principal Component Analysis and Factor Analysis in SPSS</h1><!--introduction--><!--/introduction--><h2>Contents</h2><div><ul><li><a href="#1">PCA Description</a></li><li><a href="#2">PCA Steps</a></li><li><a href="#10">PCA Output Interpretations</a></li><li><a href="#11">Rotation</a></li><li><a href="#12">PCA: What to conclude from the results</a></li></ul></div><h2>PCA Description<a name="1"></a></h2><p>Principal Component Analysis is a data mining technique used to identify "components". These are used to turn correlated variables into sets of linearly independent variables. The reason for the transformation is to include as much variation from the original variables in the uncorrelated variables. This is a way of isolating the most important parts of the data, those that explain the most variability in the original data. Having fewer variables which explain most of the variability in the dataset makes proceeding with further analysis much easier and faster. The steps to perform PCA in SPSS are outlined below and then the output will be analyzed below that.</p><h2>PCA Steps<a name="2"></a></h2><div><ol><li>Open SPSS and load in the data</li><li>From the dropdown menu, select Analyze -&gt; Dimension Reduction -&gt; Factor Analysis.</li></ol></div><p><img vspace="5" hspace="5" src="1pca.png" alt=""> </p><p>3. Next, add all of the variables into the variables box by highlighting them all and clicking the arrow pointing to the box.</p><p><img vspace="5" hspace="5" src="loaddata2.png" alt=""> </p><p>4. Now the buttons on the side will be the main focus for the next few steps. Descriptives is useful for getting a sense of the data. Select both checkboxes in the top section. In the bottom, choose Coefficients, Significance levels and KMO/Bartlett's test of sphericity. Choose to continue.</p><p><img vspace="5" hspace="5" src="fa3desc.png" alt=""> </p><p>5. Next, in the Extraction window, choose to get an unrotated factor solution and a scree plot. On the bottom, choose to extract based on eigenvalue, and put in 1. This will only show the components which account for more than one variable.</p><p><img vspace="5" hspace="5" src="faextract4.png" alt=""> </p><p>6. Rotation is not used in PCA, it is instead used in factor analysis. See the rotation section below for more information. If you are completing a factor analysis, you may select a rotation method at this stage. The loading plots can be useful, so select to receive that in the output.</p><p><img vspace="5" hspace="5" src="farot.png" alt=""> </p><p>6. It is not necessary to save the scores as variables, but opt to display factor score coefficient matrix.</p><p><img vspace="5" hspace="5" src="fascore6.png" alt=""> </p><p>7. Lastly, in the options menu, leave the missing data selections as-is. In the bottom box, choose to sort by size and exclude coefficients with absolute values lower than 0.30.</p><p><img vspace="5" hspace="5" src="faopt7.png" alt=""> </p><h2>PCA Output Interpretations<a name="10"></a></h2><div><ul><li><b>Descriptive Statistics:</b> Simply shows you the means and standard deviations of the observations you are testing. Also provides the number of cases used for each variable.</li><li><b>KMO and Bartlett's Test:</b> The KMO value ranges from zero to one, but should be greater than .6 to warrant continued analysis. Bartlett's test is designed to test whether the correlation matrix is the identity matrix. This test should also pass (reject the null hypothesis) to continue moving on. The chi-squared value should be large and the p-value small. Both should pass or you should not proceed.</li><li><b>Correlation Matrix:</b> The correlations between the different variables are shown here. With larger datasets, this will become less useful as it becomes more difficult to read.</li><li><b>Communalities:</b> This table shows the proportion of each variable accounted for by the components being used (those with eigenvalues greater than one). It can be though of as being comparable to the R-squared value when performing a regression. Initial values come from the original PCA, with extraction values coming from only the major components. These are the same values as those on the diagonal of the correlation matrix.</li><li><b>Total Variance Explained:</b> This is one of the most important outputs, showing the weight and prevalence of each component, as well as total variance accounted for by the components.</li><li><b>Scree Plot:</b> The scree plot shows the eigenvalues of each variable. Only eigenvalues over one are accepted, as they show that a variable accounts for more than its share of variance. This plot is an excellent way to visually identify eigenvalues and how significant they are. As a general rule, eigenvalues under one and connected linearly to the "tail" are not important.</li><li><b>Component Matrix:</b> This is also a very important table to look at. These values will be the loadings, the percent of variation explained by the component when it is applied to the variable. These will be further analyzed later in order to understand what the components represent.</li><li><b>Component Plot:</b> The component plot can sometimes be useful as a visual aid to identify separations in the clusters of variables. Sometimes it provides no useful information at all.</li><li><b>Component Score Covariance Matrix:</b> Since PCA makes its components orthogonal to eachother, this is simply a confirmation that none of the components vary with eachother. This should just show the identity matrix (Bartlett's test is to confirm that this matrix is the identity matrix).</li><li><b>Factor Scores:</b> These will be the values assigned to the scores of the components. The use of these is to compare the same components when evaluating variables of the same nature, but with different values.</li></ul></div><h2>Rotation<a name="11"></a></h2><h2>PCA: What to conclude from the results<a name="12"></a></h2><p>The component matrix SPSS provides will the main focus in your analysis of the PCA output. Unrotated component matrices will often be difficult to read, due to being sorted descending based on component #1. This is where rotation becomes useful, it allows sorting of the results to reveal more meaningful configurations of the component matrix.</p><p>It may be difficult to make sense of components at first, since they are indicative only of variance explained, not what variables they explain, which is where a more qualitative analysis comes into play. The eigenvalues presented in the component matrix, also known as loadings, are to be revisited, as loading values under three typically are not used because of their reliability is called into doubt. When analyzing the loadings, use the rotated values if performing factor analysis, otherwise do not.</p><p>Analyzing the components can be difficult. The idea is to find a common theme associated with the greatest loadings on an individual component. For example, if the loadings on a component represent a distinct topic that can easily discerned, it is likely that all of those variables can be explained by that one component. When interpreting loadings which are associated with two or more components, they should be given less weight in your analysis, it should not be the deciding factor in your identification of the component's meaning. This may indicate that the variable is not suitable to analyze the same topic in the future.</p><p>With the components identified, the factor scores can be utilized for comparison of multiple datasets containing the same variables but with different observations.</p><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014a</a><br></p></div><!--
##### SOURCE BEGIN #####
%% Principal Component Analysis and Factor Analysis in SPSS
%% PCA Description
% Principal Component Analysis is a data mining technique used to identify
% "components". These are used to turn correlated variables into sets of linearly
% independent variables. The reason for the transformation is to include as
% much variation from the original variables in the uncorrelated variables.
% This is a way of isolating the most important parts of the data, those
% that explain the most variability in the original data. Having fewer
% variables which explain most of the variability in the dataset makes
% proceeding with further analysis much easier and faster. The steps to
% perform PCA in SPSS are outlined below and then the output will be
% analyzed below that.
%% PCA Steps
% # Open SPSS and load in the data
% # From the dropdown menu, select Analyze -> Dimension Reduction -> Factor
% Analysis.
%%
% <<1pca.png>>
%
% 3. Next, add all of the variables into the variables box by highlighting
% them all and clicking the arrow pointing to the box.
%%
% <<loaddata2.png>>
%
% 4. Now the buttons on the side will be the main focus for the next few
% steps. Descriptives is useful for getting a sense of the data. Select
% both checkboxes in the top section. In the bottom, choose Coefficients,
% Significance levels and KMO/Bartlett's test of sphericity. Choose to
% continue.
%%
%
% <<fa3desc.png>>
%
% 5. Next, in the Extraction window, choose to get an unrotated factor
% solution and a scree plot. On the bottom, choose to extract based on
% eigenvalue, and put in 1. This will only show the components which
% account for more than one variable.
%%
%
% <<faextract4.png>>
%
% 6. Rotation is not used in PCA, it is instead used in factor analysis.
% See the rotation section below for more information. If you are
% completing a factor analysis, you may select a rotation method at this stage.
% The loading plots can be useful, so select to receive that in the output.
%%
%
% <<farot.png>>
%
% 6. It is not necessary to save the scores as variables, but opt to
% display factor score coefficient matrix.
%%
%
% <<fascore6.png>>
%
% 7. Lastly, in the options menu, leave the missing data selections as-is.
% In the bottom box, choose to sort by size and exclude coefficients with
% absolute values lower than 0.30.
%%
%
% <<faopt7.png>>
%
%% PCA Output Interpretations
% * *Descriptive Statistics:* Simply shows you the means and standard
% deviations of the observations you are testing. Also provides the number
% of cases used for each variable.
% * *KMO and Bartlett's Test:* The KMO value ranges from zero to one, but
% should be greater than .6 to warrant continued analysis. Bartlett's test
% is designed to test whether the correlation matrix is the identity
% matrix. This test should also pass (reject the null hypothesis) to
% continue moving on. The chi-squared value should be large and the p-value
% small. Both should pass or you should not proceed.
% * *Correlation Matrix:* The correlations between the different variables
% are shown here. With larger datasets, this will become less useful as it
% becomes more difficult to read.
% * *Communalities:* This table shows the proportion of each variable
% accounted for by the components being used (those with eigenvalues greater
% than one). It can be though of as being comparable to the R-squared value
% when performing a regression. Initial values come from the original PCA,
% with extraction values coming from only the major components. These are
% the same values as those on the diagonal of the correlation matrix.
% * *Total Variance Explained:* This is one of the most important outputs,
% showing the weight and prevalence of each component, as well as total variance
% accounted for by the components.
% * *Scree Plot:* The scree plot shows the eigenvalues of each variable.
% Only eigenvalues over one are accepted, as they show that a variable
% accounts for more than its share of variance. This plot is an excellent
% way to visually identify eigenvalues and how significant they are. As a
% general rule, eigenvalues under one and connected linearly to the "tail"
% are not important.
% * *Component Matrix:* This is also a very important table to look at.
% These values will be the loadings, the percent of variation explained by
% the component when it is applied to the variable. These will be further
% analyzed later in order to understand what the components represent.
% * *Component Plot:* The component plot can sometimes be useful as a visual aid to
% identify separations in the clusters of variables. Sometimes it
% provides no useful information at all.
% * *Component Score Covariance Matrix:* Since PCA makes its components
% orthogonal to eachother, this is simply a confirmation that none of the
% components vary with eachother. This should just show the identity
% matrix (Bartlett's test is to confirm that this matrix is the identity
% matrix).
% * *Factor Scores:* These will be the values assigned to the scores of the
% components. The use of these is to compare the same components when
% evaluating variables of the same nature, but with different values.
%% Rotation
% 
%% PCA: What to conclude from the results
% The component matrix SPSS provides will the main focus in your analysis
% of the PCA output. Unrotated component matrices will often be difficult
% to read, due to being sorted descending based on component #1. This is
% where rotation becomes useful, it allows sorting of the results to reveal
% more meaningful configurations of the component matrix. 
%
% It may be difficult to make sense of components at first, since they are
% indicative only of variance explained, not what variables they explain,
% which is where a more qualitative analysis comes into play. The
% eigenvalues presented in the component matrix, also known as loadings,
% are to be revisited, as loading values under three typically are not used
% because of their reliability is called into doubt. When analyzing the
% loadings, use the rotated values if performing factor analysis, otherwise
% do not.
%
% Analyzing the components can be difficult. The idea is to find a common
% theme associated with the greatest loadings on an individual component.
% For example, if the loadings on a component represent a distinct topic
% that can easily discerned, it is likely that all of those variables can
% be explained by that one component. When interpreting loadings which are
% associated with two or more components, they should be given less weight
% in your analysis, it should not be the deciding factor in your
% identification of the component's meaning. This may indicate that the
% variable is not suitable to analyze the same topic in the future. 
%
% With the components identified, the factor scores can be utilized for
% comparison of multiple datasets containing the same variables but with
% different observations. 
##### SOURCE END #####
--></body></html>